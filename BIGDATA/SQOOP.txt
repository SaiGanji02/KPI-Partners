[cloudera@quickstart ~]$ sqoop list-databases --connect jdbc:mysql://quickstart:3306/ --password cloudera --username root;
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/01/11 06:21:16 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
22/01/11 06:21:16 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
22/01/11 06:21:17 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
information_schema
cm
firehose
hue
metastore
mysql
nav
navms
oozie
retail_db
rman
sai
sentry
zeyo_db_20
zeyodb

[cloudera@quickstart ~]$ sqoop list-tables --connect jdbc:mysql://quickstart:3306/sai --password cloudera --username root;
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/01/11 06:23:31 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
22/01/11 06:23:32 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
22/01/11 06:23:33 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
KPI
[cloudera@quickstart ~]$ sqoop import --connect jdbc:mysql://quickstart:3306/sampledb --password cloudera --username root --table KPI -m 1;
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/01/11 06:27:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
22/01/11 06:27:35 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
22/01/11 06:27:37 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
22/01/11 06:27:37 INFO tool.CodeGenTool: Beginning code generation
22/01/11 06:27:40 ERROR manager.SqlManager: Error executing statement: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown database 'sampledb'
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown database 'sampledb'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)
	at com.mysql.jdbc.Util.getInstance(Util.java:360)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:978)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:870)
	at com.mysql.jdbc.MysqlIO.secureAuth411(MysqlIO.java:4332)
	at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1258)
	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2234)
	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2265)
	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2064)
	at com.mysql.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:790)
	at com.mysql.jdbc.JDBC4Connection.<init>(JDBC4Connection.java:44)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)
	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:395)
	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:325)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:247)
	at org.apache.sqoop.manager.SqlManager.makeConnection(SqlManager.java:904)
	at org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.java:52)
	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:763)
	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786)
	at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289)
	at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260)
	at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246)
	at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327)
	at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1858)
	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1657)
	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106)
	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:494)
	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:621)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)
22/01/11 06:27:40 ERROR tool.ImportTool: Import failed: java.io.IOException: No columns to generate for ClassWriter
	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1663)
	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106)
	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:494)
	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:621)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)

[cloudera@quickstart ~]$ hadoop dfs -ls
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

Found 16 items
drwx------   - cloudera cloudera          0 2022-01-10 21:43 .staging
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:09 avro_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:15 csv_dir
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 08:36 import_avro
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_avro_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:13 json_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:39 json_orc
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_orc_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:38 json_parquet
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_parquet_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:11 orc_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:14 parquet_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:11 parquet_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 13:40 part_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 14:01 part_dir2
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 09:04 zeyo_dir

[cloudera@quickstart ~]$ sqoop import --connect jdbc:mysql://quickstart:3306/sai --password cloudera --username root --table KPI -m 1;
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/01/11 06:33:48 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
22/01/11 06:33:48 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
22/01/11 06:33:49 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
22/01/11 06:33:49 INFO tool.CodeGenTool: Beginning code generation
22/01/11 06:33:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `KPI` AS t LIMIT 1
22/01/11 06:33:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `KPI` AS t LIMIT 1
22/01/11 06:33:51 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-cloudera/compile/43bd670018e346e8636d3379b4907e04/KPI.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
22/01/11 06:34:03 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/43bd670018e346e8636d3379b4907e04/KPI.jar
22/01/11 06:34:03 WARN manager.MySQLManager: It looks like you are importing from mysql.
22/01/11 06:34:03 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
22/01/11 06:34:03 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
22/01/11 06:34:03 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
22/01/11 06:34:03 INFO mapreduce.ImportJobBase: Beginning import of KPI
22/01/11 06:34:05 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
22/01/11 06:34:10 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
22/01/11 06:34:12 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/192.168.244.128:8032
22/01/11 06:34:17 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:17 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:18 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:935)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:931)
22/01/11 06:34:18 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:18 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:18 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:18 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:18 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:20 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:20 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:20 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:20 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:20 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:20 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:21 INFO db.DBInputFormat: Using read commited transaction isolation
22/01/11 06:34:21 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:21 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 06:34:21 INFO mapreduce.JobSubmitter: number of splits:1
22/01/11 06:34:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1641877638224_0002
22/01/11 06:34:23 INFO impl.YarnClientImpl: Submitted application application_1641877638224_0002
22/01/11 06:34:23 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1641877638224_0002/
22/01/11 06:34:23 INFO mapreduce.Job: Running job: job_1641877638224_0002
22/01/11 06:35:05 INFO mapreduce.Job: Job job_1641877638224_0002 running in uber mode : false
22/01/11 06:35:05 INFO mapreduce.Job:  map 0% reduce 0%
22/01/11 06:35:50 INFO mapreduce.Job:  map 100% reduce 0%
22/01/11 06:35:51 INFO mapreduce.Job: Job job_1641877638224_0002 completed successfully
22/01/11 06:35:53 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=171825
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=47
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=18917888
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=36949
		Total vcore-milliseconds taken by all map tasks=36949
		Total megabyte-milliseconds taken by all map tasks=18917888
	Map-Reduce Framework
		Map input records=3
		Map output records=3
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=260
		CPU time spent (ms)=1840
		Physical memory (bytes) snapshot=128790528
		Virtual memory (bytes) snapshot=1949466624
		Total committed heap usage (bytes)=50724864
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=47
22/01/11 06:35:53 INFO mapreduce.ImportJobBase: Transferred 47 bytes in 102.6077 seconds (0.4581 bytes/sec)
22/01/11 06:35:53 INFO mapreduce.ImportJobBase: Retrieved 3 records.
[cloudera@quickstart ~]$ hadoop dfs -ls
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

Found 17 items
drwx------   - cloudera cloudera          0 2022-01-11 06:35 .staging
drwxr-xr-x   - cloudera cloudera          0 2022-01-11 06:35 KPI
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:09 avro_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:15 csv_dir
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 08:36 import_avro
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_avro_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:13 json_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:39 json_orc
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_orc_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:38 json_parquet
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_parquet_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:11 orc_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:14 parquet_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:11 parquet_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 13:40 part_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 14:01 part_dir2
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 09:04 zeyo_dir
[cloudera@quickstart ~]$ hadoop dfs -ls KPI;
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2022-01-11 06:35 KPI/_SUCCESS
-rw-r--r--   1 cloudera cloudera         47 2022-01-11 06:35 KPI/part-m-00000
[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/KPI/part-m-00000
1,saikumar,10000
2,vamshi,20000
3,venkat,30000
[cloudera@quickstart ~]$ 
[cloudera@quickstart ~]$ 
